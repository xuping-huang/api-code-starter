# {{project.description}}

## Project prerequisites

- nodejs https://nodejs.org/en/ (v10)
- Docker, Docker Compose
{{#projectStyle.other.needTwilio}}
- Twilio Account
{{/projectStyle.other.needTwilio}}
{{#projectStyle.other.needAws}}
- AWS Account
{{/projectStyle.other.needAws}}
{{#projectStyle.db.isDynamoDB}}
- DynamoDB
{{/projectStyle.db.isDynamoDB}}
{{#projectStyle.db.isPostgres}}
- PostgresSQL 10+
{{/projectStyle.db.isPostgres}}
{{#projectStyle.db.isMssql}}
- Sql Server 2016+
{{/projectStyle.db.isMssql}}
{{#projectStyle.db.isMariadb}}
- Mariadb 10+
{{/projectStyle.db.isMariadb}}
{{#projectStyle.db.isMysql}}
- Mysql Community Server 5+
{{/projectStyle.db.isMysql}}
{{#projectStyle.db.isMongoDB}}
- MongoDB
{{/projectStyle.db.isMongoDB}}
{{#projectStyle.db.isNeo4j}}
- Neo4j
{{/projectStyle.db.isNeo4j}}

## Configuration

Configuration for the application is at `config/default.js`.
The following parameters can be set in config files or in env variables:

- LOG_LEVEL: the log level, default is 'debug'
- LOGGER_DIR: the directory stored in logs
- LOGGER_FILE: the log file name
- PORT: the server port, default is 3000
  {{#projectStyle.other.needHttps}}
- SECURE_PORT: The secure server port, default is 443.
  {{/projectStyle.other.needHttps}}
  {{#projectStyle.auth.needJwt}}
- AUTH_SECRET: The authorization secret used during token verification.
- JWT_KEY_CACHE_TIME: The token expired time.
  {{#dependencies.secure.tc-core-library-js}}
- VALID_ISSUERS: The valid issuer of tokens.
  {{/dependencies.secure.tc-core-library-js}}
  {{/projectStyle.auth.needJwt}}
  {{#projectStyle.auth.needM2M}}
  {{/projectStyle.auth.needM2M}}
  {{#projectStyle.db.isDynamoDB}}
- AWS_ACCESS_KEY_ID: The Amazon certificate key to use when connecting. Use local dynamodb you can set fake value
- AWS_SECRET_ACCESS_KEY: The Amazon certificate access key to use when connecting. Use local dynamodb you can set fake value
- AWS_REGION: The Amazon certificate region to use when connecting. Use local dynamodb you can set fake value
- AWS_IS_LOCAL: Use Amazon DynamoDB Local or server.
- AWS_DYNAMODB_ENDPOINT: The local url if using Amazon DynamoDB Local
  {{/projectStyle.db.isDynamoDB}}
  {{#projectStyle.db.isSqlite}}
- DATABASE_USERNAME: The Sqlite3 database username.
- DATABASE_PASSWORD: The Sqlite3 database password.
- DATABASE_POOL_MAX: The database connection pool max size.
  {{/projectStyle.db.isSqlite}}
  {{#projectStyle.db.isPostgres}}
- DATABASE_USERNAME: The PostgresSQL database username.
- DATABASE_PASSWORD: The PostgresSQL database password.
- DATABASE_POOL_MAX: The database connection pool max size.
- DATABASE_URL: The database link.
- DATABASE_NAME: The database name.
  {{/projectStyle.db.isPostgres}}
  {{#projectStyle.db.isMysql}}
- DATABASE_USERNAME: The mysql database username.
- DATABASE_PASSWORD: The mysql database password.
- DATABASE_POOL_MAX: The database connection pool max size.
- DATABASE_URL: The database link.
- DATABASE_NAME: The database name.
  {{/projectStyle.db.isMysql}}
  {{#projectStyle.db.isMariadb}}
- DATABASE_USERNAME: The Mariadb database username.
- DATABASE_PASSWORD: The Mariadb database password.
- DATABASE_POOL_MAX: The database connection pool max size.
- DATABASE_URL: The database link.
- DATABASE_NAME: The database name.
  {{/projectStyle.db.isMariadb}}
  {{#projectStyle.db.isMssql}}
- DATABASE_USERNAME: The SqlServer database username.
- DATABASE_PASSWORD: The SqlServer database password.
- DATABASE_POOL_MAX: The database connection pool max size.
- DATABASE_URL: The database link.
- DATABASE_NAME: The database name.
  {{/projectStyle.db.isMssql}}
  {{#dependencies.other.nodemailer}}
- EMAIL_SERVER_ACCOUNT: The email server account.
- EMAIL_SERVER_PASSWORD: The email server password.
- EMAIL_SERVER_HOST: The email server address.
- EMAIL_SERVER_HOST_PORT: The email server port.
- EMAIL_FROM: The email sender address.
  {{/dependencies.other.nodemailer}}
  {{#dependencies.other.pagination}}
- PERPAGE_DEFAULT: The default page size
- PER_PAGE_MAX: The max page size
  {{/dependencies.other.pagination}}
  {{#projectStyle.auth.needPassword}}
- PRIVATE_KEY: Private key certificate for decrypting passwords.
  {{/projectStyle.auth.needPassword}}

{{#projectStyle.db.isDynamoDB}}
## DynamoDB Setup with Docker
We will use DynamoDB setup on Docker.

Just run `docker-compose up` in docker folder. When Docker startup is complete, the table has been created in the database.

If you have already installed aws-cli in your local machine, you can use following command to verify. If not you can still verify table created following `Verify Table via awscli in Docker`.
```
aws dynamodb list-tables --endpoint-url http://localhost:8000
```

## Verify Table via awscli in Docker
1. Make sure DynamoDB are running as per instructions above.

2. Run the following commands
```
docker exec -ti dynamodb-cli sh
```
Next
```
aws dynamodb list-tables --endpoint-url http://dynamodb:8000
```
**Note:** The `endpoint-url` should be `http://dynamodb:8000`.

{{/projectStyle.db.isDynamoDB}}


{{#projectStyle.other.needKafka}}
## Local Kafka setup

- `http://kafka.apache.org/quickstart` contains details to setup and manage Kafka server,
  below provides details to setup Kafka server in Mac, Windows will use bat commands in bin/windows instead
- download kafka at `https://www.apache.org/dyn/closer.cgi?path=/kafka/1.1.0/kafka_2.11-1.1.0.tgz`
- extract out the downloaded tgz file
- go to extracted directory kafka_2.11-0.11.0.1
- start ZooKeeper server:
  `bin/zookeeper-server-start.sh config/zookeeper.properties`
- use another terminal, go to same directory, start the Kafka server:
  `bin/kafka-server-start.sh config/server.properties`
- note that the zookeeper server is at localhost:2181, and Kafka server is at localhost:9092
- use another terminal, go to same directory, create some topics:
  `bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic member.action.profile.create`
  `bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic member.action.profile.update`
- verify that the topics are created:
  `bin/kafka-topics.sh --list --zookeeper localhost:2181`,
  it should list out the created topics
- run the producer and then write some message into the console to send to the `member.action.profile.create` topic:
  `bin/kafka-console-producer.sh --broker-list localhost:9092 --topic member.action.profile.create`
  in the console, write message, one message per line:
  `{ "topic": "member.action.profile.create", "originator": "member-api", "timestamp": "2018-02-16T00:00:00", "mime-type": "application/json", "payload": { "userId": 1111, "userHandle": "handle", "email": "email@test.com", "sex": "male", "created": "2018-01-02T00:00:00", "createdBy": "admin" } }`
- optionally, use another terminal, go to same directory, start a consumer to view the messages:
  `bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic member.action.profile.create --from-beginning`
- writing/reading messages to/from other topics are similar
{{/projectStyle.other.needKafka}}


{{#projectStyle.other.needElasticSearch}}
## ElasticSearch setup

You may download ElasticSearch v6, install and run it locally.
Or to setup ES service using AWS.
Another simple way is to use docker compose:
go to docker-es folder, run `docker-compose up`
{{/projectStyle.other.needElasticSearch}}

## Local Deployment

- Install dependencies `npm install`
- Run lint `npm run lint`
- Run lint fix `npm run lint:fix`
- Start app `npm start`
  - App is running at `http://localhost:3000`
- Start app in debug mode `npm run debug`
- Clear and init db `npm run cleanData`
- Insert test data `npm run initData`
  {{#projectStyle.test.unitTest}}
- Run unit test `npm run test`
  {{/projectStyle.test.unitTest}}
  {{#projectStyle.test.e2eTest}}
- Run e2e test `npm run e2e`
  {{/projectStyle.test.e2eTest}}
  {{#projectStyle.test.needCoverage}}
- Generate coverage report for unit test `npm run cov`
  {{#projectStyle.test.e2eTest}}
- Generate coverage report for e2e test `npm run cov-e2e`
  {{/projectStyle.test.e2eTest}}
  {{/projectStyle.test.needCoverage}}

  {{#projectStyle.test.unitTest}}
## Testing
- Run `npm run test` to execute unit tests.
- RUN `npm run e2e` to execute e2e tests.
  {{/projectStyle.test.unitTest}}

## Local Deployment with Docker

{{#projectStyle.other.needElasticSearch}}
To run the Member ES Processor using docker, follow the below steps

1. Navigate to the directory `docker`

2. Rename the file `sample.api.env` to `api.env`

3. Set the required AWS credentials in the file `api.env`

4. Once that is done, run the following command

```
docker-compose up
```
{{/projectStyle.other.needElasticSearch}}

## Verification
Refer to the verification document `Verification.md`
