# {{project.description}}

## Project prerequisites

- nodejs https://nodejs.org/en/ (v10)
- Docker, Docker Compose
{{#projectStyle.other.needTwilio}}
- Twilio Account
{{/projectStyle.other.needTwilio}}
{{#projectStyle.db.isDynamoDB}}
- AWS DynamoDB
- Java 6+ (used if using runnable jar of local DynamoDB)
- Docker, Docker Compose (used if using docker of local DynamoDB)
{{/projectStyle.db.isDynamoDB}}
{{#projectStyle.db.isPostgres}}
- PostgresSQL 10+
{{/projectStyle.db.isPostgres}}
{{#projectStyle.db.isMssql}}
- Sql Server 2016+
{{/projectStyle.db.isMssql}}
{{#projectStyle.db.isMariadb}}
- Mariadb 10+
{{/projectStyle.db.isMariadb}}
{{#projectStyle.db.isMysql}}
- Mysql Community Server 5+
{{/projectStyle.db.isMysql}}
{{#projectStyle.db.isMongoDB}}
- MongoDB
{{/projectStyle.db.isMongoDB}}
{{#projectStyle.db.isNeo4j}}
- Neo4j
{{/projectStyle.db.isNeo4j}}

## Configuration

Configuration for the application is at `config/default.js` and `config/production.js`.
The following parameters can be set in config files or in env variables:

- LOG_LEVEL: the log level, default is 'debug'
- LOG.LOGGER_DIR: the directory stored in logs
- LOG.LOGGER_FILE: the log file name
- PORT: the server port, default is 3000
- API_VERSION: the base path of api version
  {{#projectStyle.other.needHttps}}
- SECURE_PORT: The secure server port, default is 443.
  {{/projectStyle.other.needHttps}}
  {{#projectStyle.auth.needJwt}}
- AUTH_SECRET: The authorization secret used during token verification.
  {{#dependencies.secure.tc-core-library-js}}
- VALID_ISSUERS: The valid issuer of tokens.
  {{/dependencies.secure.tc-core-library-js}}
  {{#projectStyle.other.needKafka}}
  {{#dependencies.secure.tc-core-library-js}}
- BUSAPI_URL: Bus API URL
- KAFKA_ERROR_TOPIC: Kafka error topic used by bus API wrapper
- KAFKA_TOPIC: the Kafka topic used by bus API wrapper, refer `config/default.js` for more details
  {{/dependencies.secure.tc-core-library-js}}
  {{/projectStyle.other.needKafka}}
  {{/projectStyle.auth.needJwt}}
  {{#projectStyle.auth.needM2M}}
- AUTH0_URL: AUTH0 URL, used to get M2M token
- AUTH0_PROXY_SERVER_URL: AUTH0 proxy server URL, used to get M2M token
- AUTH0_AUDIENCE: AUTH0 audience, used to get M2M token
- TOKEN_CACHE_TIME: AUTH0 token cache time, used to get M2M token
- AUTH0_CLIENT_ID: AUTH0 client id, used to get M2M token
- AUTH0_CLIENT_SECRET: AUTH0 client secret, used to get M2M token
- SCOPES: the configurable M2M token scopes, refer `config/default.js` for more details
  {{/projectStyle.auth.needM2M}}
  {{#dependencies.db.aws-sdk}}
- AMAZON.AWS_ACCESS_KEY_ID: The Amazon certificate key to use when connecting. Use local dynamodb you can set fake value
- AMAZON.AWS_SECRET_ACCESS_KEY: The Amazon certificate access key to use when connecting. Use local dynamodb you can set fake value
- AMAZON.AWS_REGION: The Amazon certificate region to use when connecting. Use local dynamodb you can set fake value
- AMAZON.AWS_IS_LOCAL: Use Amazon DynamoDB Local or server.
  {{#projectStyle.db.isDynamoDB}}
- AMAZON.AWS_DYNAMODB_ENDPOINT: The local url if using Amazon DynamoDB Local
- AMAZON.DYNAMODB_THROUGHPUT_READ: The AWS DynamoDB read capacity units
- AMAZON.DYNAMODB_THROUGHPUT_WRITE: The AWS DynamoDB write capacity units
  {{/projectStyle.db.isDynamoDB}}
  {{/dependencies.db.aws-sdk}}
  {{#projectStyle.db.isSqlite}}
- DATABASE.USERNAME: The Sqlite3 database username.
- DATABASE.PASSWORD: The Sqlite3 database password.
- DATABASE.POOL_MAX: The database connection pool max size.
  {{/projectStyle.db.isSqlite}}
  {{#projectStyle.db.isPostgres}}
- DATABASE.USERNAME: The PostgresSQL database username.
- DATABASE.PASSWORD: The PostgresSQL database password.
- DATABASE.POOL_MAX: The database connection pool max size.
- DATABASE.URL: The database link.
- DATABASE.NAME: The database name.
  {{/projectStyle.db.isPostgres}}
  {{#projectStyle.db.isMysql}}
- DATABASE.USERNAME: The mysql database username.
- DATABASE.PASSWORD: The mysql database password.
- DATABASE.POOL_MAX: The database connection pool max size.
- DATABASE.URL: The database link.
- DATABASE.NAME: The database name.
  {{/projectStyle.db.isMysql}}
  {{#projectStyle.db.isMariadb}}
- DATABASE.USERNAME: The Mariadb database username.
- DATABASE.PASSWORD: The Mariadb database password.
- DATABASE.POOL_MAX: The database connection pool max size.
- DATABASE.URL: The database link.
- DATABASE.NAME: The database name.
  {{/projectStyle.db.isMariadb}}
  {{#projectStyle.db.isMssql}}
- DATABASE.USERNAME: The SqlServer database username.
- DATABASE.PASSWORD: The SqlServer database password.
- DATABASE.POOL_MAX: The database connection pool max size.
- DATABASE.URL: The database link.
- DATABASE.NAME: The database name.
  {{/projectStyle.db.isMssql}}
  {{#projectStyle.other.needS3}}
- AMAZON.S3_API_VERSION: the AWS S3 api version
- AMAZON.ATTACHMENT_S3_BUCKET: the AWS S3 bucket to store attachments
  {{/projectStyle.other.needS3}}
  {{#projectStyle.other.needElasticSearch}}
- ES.HOST: Elasticsearch host
- ES.API_VERSION: Elasticsearch API version
- ES.ES_INDEX: Elasticsearch index name
- ES.ES_TYPE: Elasticsearch index type
  {{/projectStyle.other.needElasticSearch}}
  {{#dependencies.other.nodemailer}}
- EMAIL.ACCOUNT: The email server account.
- EMAIL.PASSWORD: The email server password.
- EMAIL.HOST: The email server address.
- EMAIL.WHOST_PORT: The email server port.
- EMAIL.FROM: The email sender address.
  {{/dependencies.other.nodemailer}}
  {{#dependencies.other.pagination}}
- PAGE.PERPAGE_DEFAULT: The default page size
- PAGE.PER_PAGE_MAX: The max page size
  {{/dependencies.other.pagination}}
  {{#projectStyle.auth.needPassword}}
  {{/projectStyle.auth.needPassword}}
{{#projectStyle.test.e2eTest}}

Test configuration is at `config/test.js`. You don't need to change them.
The following test parameters can be set in config file or in env variables:

- WAIT_TIME: the wait time before test running
{{/projectStyle.test.e2eTest}}
{{#projectStyle.db.isDynamoDB}}

## Local DynamoDB setup (Optional)

This page `https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.html` provides several ways to deploy local DynamoDB.

If you want to use runnable jar of local DynamoDB:

- see `https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.DownloadingAndRunning.html` for details
- download the local DynamoDB of your region
- extract out the downloaded archive
- ensure Java 6+ is installed
- in the extracted folder, run `java -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar -sharedDb`
- local DynamoDB is running at `http://localhost:8000`

If you want to use docker of local DynamoDB:

- see `https://hub.docker.com/r/amazon/dynamodb-local` for details
- you may go to `docker` folder, and run `docker-compose up` to start local DynamoDB
- local DynamoDB is running at `http://localhost:8000`

## AWS DynamoDB setup

Properly configure AMAZON.AWS_ACCESS_KEY_ID, AMAZON.AWS_SECRET_ACCESS_KEY, AMAZON.AWS_REGION, AMAZON.AWS_IS_LOCAL
in config file or via environment variables. You may create tables using below `npm run create-tables` command.
{{/projectStyle.db.isDynamoDB}}
{{#projectStyle.other.needKafka}}

## Local Kafka setup

- `http://kafka.apache.org/quickstart` contains details to setup and manage Kafka server,
  below provides details to setup Kafka server in Mac, Windows will use bat commands in bin/windows instead
- download kafka at `https://www.apache.org/dyn/closer.cgi?path=/kafka/1.1.0/kafka_2.11-1.1.0.tgz`
- extract out the downloaded tgz file
- go to extracted directory kafka_2.11-0.11.0.1
- start ZooKeeper server:
  `bin/zookeeper-server-start.sh config/zookeeper.properties`
- use another terminal, go to same directory, start the Kafka server:
  `bin/kafka-server-start.sh config/server.properties`
- note that the zookeeper server is at localhost:2181, and Kafka server is at localhost:9092
- use another terminal, go to same directory, create some topics:
  `bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic member.action.profile.create`
  `bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic member.action.profile.update`
- verify that the topics are created:
  `bin/kafka-topics.sh --list --zookeeper localhost:2181`,
  it should list out the created topics
- run the producer and then write some message into the console to send to the `member.action.profile.create` topic:
  `bin/kafka-console-producer.sh --broker-list localhost:9092 --topic member.action.profile.create`
  in the console, write message, one message per line:
  `{ "topic": "member.action.profile.create", "originator": "member-api", "timestamp": "2018-02-16T00:00:00", "mime-type": "application/json", "payload": { "userId": 1111, "userHandle": "handle", "email": "email@test.com", "sex": "male", "created": "2018-01-02T00:00:00", "createdBy": "admin" } }`
- optionally, use another terminal, go to same directory, start a consumer to view the messages:
  `bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic member.action.profile.create --from-beginning`
- writing/reading messages to/from other topics are similar
{{/projectStyle.other.needKafka}}
{{#projectStyle.other.needElasticSearch}}

## ElasticSearch setup

You may download ElasticSearch v6, install and run it locally.
Or to setup ES service using AWS.
Another simple way is to use docker compose:
go to docker-es folder, run `docker-compose up`
{{/projectStyle.other.needElasticSearch}}

## Local Deployment

- Install dependencies `npm install`
- Run lint `npm run lint`
- Run lint fix `npm run lint:fix`
- Start app `npm start`
  - App is running at `http://localhost:3000`
- Start app in debug mode `npm run debug`
- To delete DynamoDB table if needed `npm run delete-tables`
- To create DynamoDB table if needed `npm run create-tables`
- Clear and init db `npm run clean-data`
- Insert test data `npm run init-data`
  {{#projectStyle.test.unitTest}}
- Run unit test `npm run test`
  {{/projectStyle.test.unitTest}}
  {{#projectStyle.test.e2eTest}}
- Run e2e test `npm run e2e`
  {{/projectStyle.test.e2eTest}}
  {{#projectStyle.test.needCoverage}}
- Generate coverage report for unit test `npm run cov`
  {{#projectStyle.test.e2eTest}}
- Generate coverage report for e2e test `npm run cov-e2e`
  {{/projectStyle.test.e2eTest}}
  {{/projectStyle.test.needCoverage}}

  {{#projectStyle.test.unitTest}}
## Testing

Tables should be created before running tests.

- Run `npm run test` to execute unit tests.
- RUN `npm run e2e` to execute e2e tests.
  {{/projectStyle.test.unitTest}}
{{#projectStyle.other.needElasticSearch}}

## Local Deployment with Docker
To run the Member ES Processor using docker, follow the below steps

1. Navigate to the directory `docker`

2. Rename the file `sample.api.env` to `api.env`

3. Set the required AWS credentials in the file `api.env`

4. Once that is done, run the following command

```
docker-compose up
```
{{/projectStyle.other.needElasticSearch}}

## Notes

- swagger is at `doc/swagger.yaml`, you may check it using `http://editor.swagger.io/`
- all JWT tokens provided in tests are created in `https://jwt.io` with secret `mysecret`
- the local db setup, db tables management are provided to make review easier

## Verification
Refer to the verification document `Verification.md`
